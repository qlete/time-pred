\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{custom}

\title{LINMA 2120 - Seminars in Applied Mathematics \\
        High dimensional time-series prediction with missing values}
\author{Quentin Lété}
\date{November 2017}

\begin{document}

\maketitle

\section{Introduction}

Let us first give a definition of time series to explain the formal context in which we work. \\

\theoremstyle{definition}
\begin{definition}{Time series}
A time series is a sequence of data indexed by the time.
\end{definition}

A time series is thus a realization of a stochastic process which can be seen as a family of random variables indexed by the time. \\

We also define here a property of a stochastic process that will be useful later : the Markow property. \\

\theoremstyle{definition}
\begin{definition}{Markov property}
We say that $(Y_t)_{t \ge 0}$ is a Markov chain if for all $t > 0$
$$\pi(y_t|y_{1:t-1}) = \pi(y_t|y_{t-1})$$
where $\pi$ is the probability.
\end{definition}

This means that all the information about the past is completely carried out in the $Y_t$ at each step. With Markov chains, the joint distribution of observations take a fairly simple form :

$$\pi(y_{1:t}) = \pi(y_1) \cdot \prod_{j=2}^2 \pi(y_j|y_{j-1})$$

\section*{Classical methods}
In this section, we present classical methods used for time series forecasting.
These classical methods include AR (autoregressive) models and DLM (dynamic linear models) and are used for prediction.
Let us explain sequentially these models.

\subsection*{Autoregressive models}
As explained in \cite{pmlr-v37-anava15}, the idea of this model is to represent each observation as a noisy linear combination of previous observations.
Formally, if $X_t$ is the measurement at time $t$, the AR(p) model prametrized by the lag $p$ and the coefficient vector $\alpha \in \mathbb{R}^p$ can be written as

$$X_t = \sum_{k=1}^p \alpha_k X_{t-k} + \epsilon_t$$

where $\{ \epsilon_t \}_t \in \mathbb{Z}$ is a white noise.

This moedel is motivated by a thorem due to Wold that states that a stationary process can be represented by a MA($\infty$) model, that is

$$X_t = \sum_{i=1}^{\infty} \beta_i \epsilon_{t-i} + \epsilon_t$$

where $\sum_{i=1}^\infty \beta_i < \infty$ and $\{\epsilon_t\}_{t \in \mathbb{Z}}$ have zero mean and equal variance. And if $\{ X_t \}_t^\infty$ is invertible, we also have that it can be represented by an AR$(\infty)$ model, that is

$$X_t = \sum_{i=1}^{\infty} \alpha_i X_{t-i} + \epsilon_t$$

With this theorem, it seems natural to use AR$(p)$ models for prediction.

\subsection*{Dynamic Linear Models}
DLM is a simpler model of a more general framework called state space models. \\
\begin{definition}{State space model}
A state space model consist of two time series : a $\mathbb{R}^p$-valued time series $\{\theta_t\}$ and a a $\mathbb{R}^m$-valued time series $\{Y_t\}$ satisfying the following assumptions :

\begin{itemize}
        \item ($\theta_t$) is a Markov chain
        \item Conditionnaly on $(\theta_t)$, the $Y_t$'s are independent and the $Y_t$'s depends only on $(\theta_t)$
\end{itemize}
\end{definition}

The idea inder this model is that the seris $Y_t$ is determined by a latent process $\theta_t$. The $\theta_t$'s usually represent all the observable physical variables that have an influence on the variable of interest. For instance, if we try to predict the price of electricity in the future, the latent variables could be the temperature, the wind speed, the sunshine, ... Note that this ($\theta_t$) is assumed to be a Markov chain. \\

In general, a state space model consists in two equations : an \textbf{obvservation equation} which gives $Y_t$ in function of $\theta_t$ at each step and an \textbf{evolution equation} which gives $\theta_t$ in function of $\theta_{t-1}$. Both equations are pertubed by noise. This can be written as

$$Y_t = f_t(\theta_t, v_t)$$
$$\theta_t = g_t(\theta_{t-1}, w_t)$$

To be complete we also have to specify the prior distribution for $\theta_0$. \\

With that general state space model defined, it is possible to define the DLM which is a special case of it with linear functions and Gaussian noise. \\

\theoremstyle{definition}
\begin{definition}{DLM}
A dynamic linear model (DLM) is specified by a Normal prior distribution for the p-dimensional state vector at time t = 0,
$$\theta_0 \sim \mathcal{N}_p(m_0, C_0)$$
together with a pair of equations for each time $t \ge 1$,

$$Y_t = F_t\theta_t + v_t, \hspace{3cm} v_t \sim \mathcal{N}_m(0, V_t)$$
$$\theta_t = G_t\theta_{t-1} + w_t, \hspace{3cm} v_t \sim \mathcal{N}_p(0, W_t)$$

where $F_t$ and $G_t$ are known matrices of order $m \times p$ and $p \times p$ respectively.
\end{definition}

\subsubsection*{Forecasting}

Based on this model, how can we predict the future observation ? The problem is solved by use of the \textbf{Kalman filter}. This filter is based on the idea that, due to the markovian property of the state vector and the assumption of conditional independence of the observations, the density of the state and the observation knowing their previous values can be computed recursively. Moreover, because the equations are linear and the noise and prior distribution are Gaussian, it is possible to prove that the random vector $(\theta_0, ..., \theta_t, Y_0, ..., Y_t)$ follows a mutlivariate Gaussian distribution. Therefore, only the mean and covariance matrix has to be computed. The Kalman filter gives the recursion equation for them : \\

\begin{theorem}
Let $\mathcal{D}_t$ be the information provided by the first $t$ observations $Y_1, ..., Y_t$.
Then, if
$$\theta_{t-1} \sim \mathcal{N}(m_{t-1}, C_{t-1}),$$
where $t \ge 1$, then

\begin{enumerate}[(a)]

\item the one-step-ahead predictive density of $\theta_t$ given $\mathcal{D}_{t-1}$ is Gaussian with parameters

$$a_t = E(\theta_t|\mathcal{D}_{t-1}) = G_tm_{t-1}$$
$$R_t = \text{Var}(\theta_t|\mathcal{D}_{t-1}) = G_tC_{t-1}G_t' + W_t$$

\item the one-step-ahead predictive density of $Y_t$ given $\mathcal{D}_{t-1}$ is Gaussian with parameters

$$f_t = E(Y_t|\mathcal{D}_{t-1}) = F_ta_t$$
$$Q_t = \text{Var}(Y_t|\mathcal{D}_{t-1}) = F_tR_tF_t' + V_t$$

\item the filtering density of $\theta_t$ given $\mathcal{D}_t$ is Gaussian with parameters

$$m_t = E(\theta_t|\mathcal{D}_{t}) = a_t + R_tF_t'Q_t^{-1}e_t$$
$$C_t = \text{Var}(\theta_t|\mathcal{D}_{t}) = R_t - R_tF_t'Q_t^{-1}F_tR_t$$

where $e_t = Y_t-f_t$ is the forecast error.

\end{enumerate}

This theorem gives us everything we need to predict the behaviour of the observations for a time series modeled by a DLM. \\

It can be seen that the complexity of this algorithm is $\mathcal{O}(kn^2T + k^3T)$.

\end{theorem}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
